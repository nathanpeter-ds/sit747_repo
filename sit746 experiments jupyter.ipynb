{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe823e90-2aa5-4e7d-9aa2-01ea28dddb06",
   "metadata": {},
   "source": [
    "# Sit746 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f082d-ada2-43a2-b5f9-46936d25b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef25dc1-41c0-48b3-9101-47f8884f8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for PER\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PER:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action,reward, next_state, done, n_steps_arr, error):\n",
    "        priority = (abs(error)+1e-5)**self.alpha\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state,action,reward,next_state,done,n_steps_arr))\n",
    "            self.priorities.append(priority)\n",
    "        else:\n",
    "            self.buffer[self.position] = (state,action,reward,next_state,done,n_steps_arr)\n",
    "            self.priorities[self.position] = priority\n",
    "        self.position = (self.position+1)%self.capacity\n",
    "        \n",
    "    def sample(self, batch_size, beta=0.4, device=\"cpu\"):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None  # Return None instead of doing nothing (prevents errors)\n",
    "        probs = np.array(self.priorities)/sum(self.priorities)\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)  \n",
    "        batch = [self.buffer[i] for i in indices]  # Extract batch from buffer\n",
    "    \n",
    "        # Efficiently unpack and convert to tensors\n",
    "        states, actions, rewards, next_states, dones, n_steps_arr = zip(*batch)\n",
    "        weights = (len(self.buffer)*probs[indices])**(-beta)\n",
    "        weights /= weights.max()\n",
    "    \n",
    "        return (\n",
    "                torch.tensor(np.array(states), dtype=torch.float32, device=device),\n",
    "                torch.tensor(np.array(actions), dtype=torch.long, device=device),\n",
    "                torch.tensor(np.array(rewards), dtype=torch.float32, device=device),\n",
    "                torch.tensor(np.array(next_states), dtype=torch.float32, device=device),\n",
    "                torch.tensor(np.array(dones), dtype=torch.float32, device=device), \n",
    "                torch.tensor(np.array(n_steps_arr), dtype=torch.float32, device = device),\n",
    "                torch.tensor(weights, dtype=torch.float32, device=device),\n",
    "                indices\n",
    "            )\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = (abs(error)+1e-5)**self.alpha\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b63c9-2b7b-4e04-9901-bbbec1c9d4f0",
   "metadata": {},
   "source": [
    "Below we import all the train and test data sets for the main experiments and generalization crypto experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297dd7b-7eba-458e-9d64-9b3d76dd2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing yahoo datasets\n",
    "import yfinance as yf\n",
    "#apple train and validation\n",
    "apple_train = yf.download('aapl', '2007-01-01','2017-01-01',multi_level_index=False,progress=False)\n",
    "apple_valid = yf.download('aapl', '2018-01-01','2021-01-01',multi_level_index=False,progress=False)\n",
    "#ge train and validation\n",
    "ge_train = yf.download('ge', '2007-01-01','2017-01-01',multi_level_index=False,progress=False)\n",
    "ge_valid = yf.download('ge', '2018-01-01','2021-01-01',multi_level_index=False,progress=False)\n",
    "\n",
    "#DJI train and validation\n",
    "dji_train = yf.download('^dji', '2007-01-01','2017-01-01',multi_level_index=False,progress=False)\n",
    "dji_valid = yf.download('^dji', '2018-01-01','2021-01-01',multi_level_index=False,progress=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79aec4-e3d9-44b6-9922-169b1afa6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crypto btc\n",
    "btc_train = yf.download('btc-usd', '2018-01-01','2024-01-01',multi_level_index=False,progress=False)\n",
    "btc_valid = yf.download('btc-usd', '2024-01-01','2025-07-01',multi_level_index=False,progress=False)\n",
    "\n",
    "#crypto eth\n",
    "eth_train = yf.download('eth-usd', '2018-01-01','2024-01-01',multi_level_index=False,progress=False)\n",
    "eth_valid = yf.download('eth-usd', '2024-01-01','2025-07-01',multi_level_index=False,progress=False)\n",
    "\n",
    "#crypto sol, bnb\n",
    "sol_valid = yf.download('sol-usd', '2024-01-01','2025-07-01',multi_level_index=False,progress=False)\n",
    "bnb_valid = yf.download('bnb-usd', '2024-01-01','2025-07-01',multi_level_index=False,progress=False)\n",
    "xrp_valid = yf.download('xrp-usd', '2024-01-01','2025-07-01',multi_level_index=False,progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264a137-81e4-4d7b-a382-f7f918bc8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "def kalman_filter(x, col):\n",
    "    x = x.copy()\n",
    "    \n",
    "    for i in range(len(col)):\n",
    "        prices = x[col[i]]\n",
    "        y = prices.values\n",
    "        \n",
    "        #set up a simple random-walk + noise model\n",
    "        kf = KalmanFilter(\n",
    "            transition_matrices = [1],      # x_t = x_{t-1} + process_noise\n",
    "            observation_matrices = [1],     # y_t = x_t + obs_noise\n",
    "            transition_covariance = 0.01,   # tune: process noise variance\n",
    "            observation_covariance = 1.0,   # tune: observation noise variance\n",
    "            initial_state_mean = y[0],\n",
    "            initial_state_covariance = 1.0,\n",
    "            random_state = 42\n",
    "        )\n",
    "        \n",
    "        #Run the filter + smoother\n",
    "        state_means, state_covs = kf.smooth(y)\n",
    "        \n",
    "        #Put the smoothed values back into a Series\n",
    "        x[f'kalman_{col[i]}'] = state_means.flatten()\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ea6d9-695a-4776-a169-5c69d03c1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykalman\n",
    "import matplotlib\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(yf.__version__)\n",
    "print(torch.__version__)\n",
    "print(pykalman.__version__)\n",
    "print(matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddbba4-7e15-4686-9cb3-b42c7554d285",
   "metadata": {},
   "source": [
    "Define a preprocess function that does all feature engineering. In this case it only applies Kalman filter, but in the future if there were extra steps the functions would all be added into this to streamline the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21305a0e-0fd1-455f-8cc0-0589148c2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = x.copy()\n",
    "    cols = x.columns\n",
    "    x = kalman_filter(x,cols)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff5b1d-e5f1-4ce9-bd45-6f8f237d378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_t_pre = preprocess(apple_train)\n",
    "app_v_pre = preprocess(apple_valid)\n",
    "ge_t_pre = preprocess(ge_train)\n",
    "ge_v_pre = preprocess(ge_valid)\n",
    "dji_t_pre = preprocess(dji_train)\n",
    "dji_v_pre = preprocess(dji_valid)\n",
    "btc_t_pre = preprocess(btc_train)\n",
    "btc_v_pre = preprocess(btc_valid)\n",
    "eth_v_pre = preprocess(eth_valid)\n",
    "sol_v_pre = preprocess(sol_valid)\n",
    "bnb_v_pre = preprocess(bnb_valid)\n",
    "xrp_v_pre = preprocess(xrp_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0bd82-1d53-4182-877c-69cc8a45f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all preprocessed data to csv files for future work incase Yahoo Finance is down\n",
    "app_t_pre.to_csv('apple_train.csv') \n",
    "app_v_pre.to_csv('apple_valid.csv') \n",
    "ge_t_pre.to_csv('ge_train.csv')  \n",
    "ge_v_pre.to_csv('ge_valid.csv')  \n",
    "dji_t_pre.to_csv('dji_train.csv')  \n",
    "dji_v_pre.to_csv('dji_valid.csv')  \n",
    "btc_t_pre.to_csv('btc_train.csv')  \n",
    "btc_v_pre.to_csv('btc_valid.csv')  \n",
    "eth_v_pre.to_csv('eth_valid.csv')  \n",
    "sol_v_pre.to_csv('sol_valid.csv')  \n",
    "bnb_v_pre.to_csv('bnb_valid.csv')  \n",
    "xrp_v_pre.to_csv('xrp_valid.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991761b-dc85-4726-8314-96e951cc3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SingleStockKFohlcvCuilogr as envv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44da3a-b728-4d81-bb4a-dc50ff685772",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 20 #window length\n",
    "vars = 5 #number of variables OHLCV\n",
    "trans = 0.001 #0.1% transaction cost\n",
    "#create all validation environments for main and generalization experiments\n",
    "env_v = envv2.SingleStockTraderKFohlcvCuilogr(btc_v_pre, lookback=lb, transaction = trans)\n",
    "env_v1 = envv2.SingleStockTraderKFohlcvCuilogr(eth_v_pre, lookback=lb, transaction = trans)\n",
    "env_v2 = envv2.SingleStockTraderKFohlcvCuilogr(sol_v_pre, lookback=lb, transaction = trans)\n",
    "env_v3 = envv2.SingleStockTraderKFohlcvCuilogr(bnb_v_pre, lookback=lb, transaction = trans)\n",
    "env_v4 = envv2.SingleStockTraderKFohlcvCuilogr(xrp_v_pre, lookback=lb, transaction = trans)\n",
    "env_v5 = envv2.SingleStockTraderKFohlcvCuilogr(app_v_pre, lookback=lb, transaction = trans)\n",
    "env_v6 = envv2.SingleStockTraderKFohlcvCuilogr(ge_v_pre, lookback=lb, transaction = trans)\n",
    "env_v7 = envv2.SingleStockTraderKFohlcvCuilogr(dji_v_pre, lookback=lb, transaction = trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236d282-db0d-4dda-9c4c-e2c8089841b3",
   "metadata": {},
   "source": [
    "Below is the standardization function that subtracts mean and divides by standard deviation before the inputs go into the network. The 1e-8 term is to prevent division of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80a8ce-f8a7-44b0-b6ae-c106b0f11f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(state):\n",
    "    market = state.copy()\n",
    "    market = (market-market.mean(axis=0))/(market.std(axis=0)+1e-8)\n",
    "    return market.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56b8a-caa6-4a88-a1fd-d70814385ad5",
   "metadata": {},
   "source": [
    "Below is the dual CNN layers with C51. The multi-step learning and PER is built into the full algorithm after this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b0a5f-db1b-408e-a2f4-5802de402d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class C51CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lb: int,\n",
    "        num_vars: int,\n",
    "        hidden: int = 64,\n",
    "        hidden2: int = 32,\n",
    "        num_atoms: int = 51,\n",
    "        Vmin: float = -10.0,\n",
    "        Vmax: float = 10.0,\n",
    "        dropout_p: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lb = lb\n",
    "        self.num_vars = num_vars\n",
    "        self.action_dim = action_dim\n",
    "        self.num_atoms = num_atoms\n",
    "        self.Vmin, self.Vmax = Vmin, Vmax\n",
    "        self.delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "        self.register_buffer(\"support\", torch.linspace(Vmin, Vmax, num_atoms))\n",
    "\n",
    "        # 1) CNN encoder\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(num_vars, hidden, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(hidden, hidden2, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "        # 2) MLP head with dropout\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden2, action_dim * num_atoms)\n",
    "\n",
    "    def forward(self, s: torch.Tensor):\n",
    "        B = s.size(0)       \n",
    "\n",
    "        # reshape market -> (B, num_vars, lb)\n",
    "        mkt = s.view(B, self.lb, self.num_vars).permute(0, 2, 1)\n",
    "\n",
    "        # 1) CNN encoder\n",
    "        x = self.conv_net(mkt)  \n",
    "\n",
    "        # 2) MLP head\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 3) Distributional head\n",
    "        logits = self.fc2(x).view(B, self.action_dim, self.num_atoms)\n",
    "        probs = F.softmax(logits, dim=2)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6212d1-863b-45d3-8569-b287fd29a546",
   "metadata": {},
   "source": [
    "## Main Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6a7bc-ba5f-46a7-90ee-ed2650a536c7",
   "metadata": {},
   "source": [
    "### 1. DJI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ee366-2262-4d13-9500-9626eaf13231",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(dji_t_pre, lookback=lb, transaction =trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1850a55-4c13-489c-a983-3672b5fe323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1)  # shape: [batch, 1]\n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  # expected Q-values\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2)  # [batch, action_dim]\n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions]  # [batch, num_atoms]\n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  # [batch, action_dim, num_atoms]\n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v7.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v7.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"dji_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b3822-3f8b-431b-9fc1-0019134c61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward dji', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'dji_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b674c7-4f10-44ec-af54-19f47e4da2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = C51CNN(state_dim, action_dim,lb,vars).to(device)\n",
    "\n",
    "# 2) Load the state dict\n",
    "ckpt = torch.load(\"t11_policy_ep050.pth\", map_location=device)\n",
    "policy.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f2cb0-d227-42af-b05e-177dcbf9f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "#calculate confidence interval\n",
    "def calc_ci(x,n=10):\n",
    "    xbar = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    t_c = t.ppf(1-0.05/2,n-1)\n",
    "    width = t_c*sd/np.sqrt(n)\n",
    "    return xbar.round(2), width.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d866ed-7f60-4040-9ff7-affdd90229dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CI based graph\n",
    "def shaded_region(ports, bnh, final, ticker):\n",
    "    '''\n",
    "    ports are all the portfolios over the 10 runs\n",
    "    bnh is the buy and hold portfolio series\n",
    "    final are the final values of the algorithm portfolios from ports\n",
    "    '''\n",
    "    mean_port = (pd.DataFrame(ports).T).mean(axis=1)\n",
    "    width = calc_ci(final)[1]\n",
    "    lower = mean_port-width\n",
    "    upper = mean_port+width\n",
    "    ind = bnh.index\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(ind, mean_port, label=f'{ticker}',c='b')\n",
    "    plt.plot(ind,lower,alpha=0)\n",
    "    plt.plot(ind, upper,alpha=0)\n",
    "    plt.fill_between(ind,lower,upper,alpha=0.1, color='blue')\n",
    "    plt.plot(ind,bnh.values, label='bnh', c='orange')\n",
    "    plt.xlabel('Test Set Date Range', fontsize=16)\n",
    "    plt.ylabel('Portfolio Value', fontsize=16)\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig(f'{ticker}_port_graph.png', dpi=1000)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f81a5-75f9-42ad-a9f5-a9251f796927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation functions\n",
    "def sharpe_ratio(port):\n",
    "    rets = port.pct_change().dropna()\n",
    "    sharpe = np.sqrt(252)*(rets.mean())/(rets.std())\n",
    "    return round(sharpe,2)\n",
    "\n",
    "def annual_return(port):\n",
    "    growth = port.iloc[-1]/port.iloc[0]\n",
    "    n = len(port)\n",
    "    ar = growth**(365/n)-1\n",
    "    return 100*round(ar,4)\n",
    "\n",
    "def profit_return(x, x0=500000):\n",
    "    profit = x.iloc[-1] - x0\n",
    "    pr = 100*(profit/x0)\n",
    "    return round(profit,2), round(pr,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343ee4d-9028-45f0-8322-71fe928c16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_dji = []\n",
    "pr_dji = []\n",
    "sr_dji = []\n",
    "ar_dji= []\n",
    "ports_dji = []\n",
    "final_dji = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v7.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Get the distribution output: shape [1, action_dim, num_atoms]\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v7.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_dji = pd.DataFrame(episode_history)\n",
    "    port_dji = pr_v_dji.Portfolio\n",
    "    profit_d = profit_return(port_dji)[0]\n",
    "    pr_d = profit_return(port_dji)[1]\n",
    "    sr_d = sharpe_ratio(port_dji)\n",
    "    ar_d = annual_return(port_dji)\n",
    "    profit_dji.append(profit_d)\n",
    "    pr_dji.append(pr_d)\n",
    "    sr_dji.append(sr_d)\n",
    "    ar_dji.append(ar_d)\n",
    "    ports_dji.append(port_dji)\n",
    "    final_dji.append(port_dji.iloc[-1])\n",
    "    \n",
    "bnhs_dji = (500000 * np.cumprod(1 + dji_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_dji = bnhs_dji.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_dji-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_dji))\n",
    "print('bnh ar', annual_return(bnhs_dji))\n",
    "\n",
    "print('dji profit', calc_ci(profit_dji))\n",
    "print('dji pr', calc_ci(pr_dji))\n",
    "print('dji sr', calc_ci(sr_dji))\n",
    "print('dji ar', calc_ci(ar_dji))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b473bd-798a-4409-b958-2a744fc06651",
   "metadata": {},
   "outputs": [],
   "source": [
    "shaded_region(ports_dji,bnhs_dji,final_dji,'dji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac073afc-1c30-4d79-bb6f-3ad8aec47720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "dji_trial_runs = pd.DataFrame(ports_dji).T\n",
    "dji_trial_runs.to_csv('dji_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63df6d1-5514-4bfc-85a0-96e77f3545bf",
   "metadata": {},
   "source": [
    "### 2. Apple Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20a20b-b1dd-4047-8120-7cace66925fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(app_t_pre, lookback=lb, transaction =trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988ea86-57dd-4de1-bc1e-8c9de48c815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "# Scheduler for learning rate\n",
    "# at the top\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=75, gamma=0.5)\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1) \n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  # expected Q-values\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2) \n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions]  # [batch, num_atoms]\n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  \n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v5.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v5.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"app_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdd444-dc9b-47e8-b180-ffee6347693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward aapl', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'aapl_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ab231-752e-4e10-a6b5-7bd399d05812",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_app = []\n",
    "pr_app = []\n",
    "sr_app = []\n",
    "ar_app = []\n",
    "ports_app = []\n",
    "final_app = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v5.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Get the distribution output: shape [1, action_dim, num_atoms]\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v5.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_app = pd.DataFrame(episode_history)\n",
    "    port_app = pr_v_app.Portfolio\n",
    "    profit_a = profit_return(port_app)[0]\n",
    "    pr_a = profit_return(port_app)[1]\n",
    "    sr_a = sharpe_ratio(port_app)\n",
    "    ar_a = annual_return(port_app)\n",
    "    profit_app.append(profit_a)\n",
    "    pr_app.append(pr_a)\n",
    "    sr_app.append(sr_a)\n",
    "    ar_app.append(ar_a)\n",
    "    ports_app.append(port_app)\n",
    "    final_app.append(port_app.iloc[-1])\n",
    "    \n",
    "bnhs_app = (500000 * np.cumprod(1 + apple_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_app = bnhs_app.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_app-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_app))\n",
    "print('bnh ar', annual_return(bnhs_app))\n",
    "\n",
    "print('apple profit', calc_ci(profit_app))\n",
    "print('apple pr', calc_ci(pr_app))\n",
    "print('apple sr', calc_ci(sr_app))\n",
    "print('apple ar', calc_ci(ar_app))\n",
    "\n",
    "shaded_region(ports_app,bnhs_app,final_app,'aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2c4c0-d91b-46d8-9290-0edfd5825a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "app_trial_runs = pd.DataFrame(ports_app).T\n",
    "app_trial_runs.to_csv('app_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39a985-b677-40d5-96d6-ee9ef3a13f7c",
   "metadata": {},
   "source": [
    "### 3. GE Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb41809-695e-4551-98f1-b74192217c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(ge_t_pre, lookback=lb, transaction =trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c335f-9319-4117-9172-0fb1d337ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1)  \n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  # expected Q-values\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2)  \n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions] \n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  # [batch, action_dim, num_atoms]\n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "           \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v6.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v6.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"ge_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53208ff8-7f90-4f70-ad5a-985ac317a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward ge', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'ge_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0527274-cd8c-4d8c-830c-50c04ba1f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_ge = []\n",
    "pr_ge = []\n",
    "sr_ge = []\n",
    "ar_ge= []\n",
    "ports_ge = []\n",
    "final_ge = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v6.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Get the distribution output: shape [1, action_dim, num_atoms]\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v6.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_ge = pd.DataFrame(episode_history)\n",
    "    port_ge = pr_v_ge.Portfolio\n",
    "    profit_g = profit_return(port_ge)[0]\n",
    "    pr_g = profit_return(port_ge)[1]\n",
    "    sr_g = sharpe_ratio(port_ge)\n",
    "    ar_g = annual_return(port_ge)\n",
    "    profit_ge.append(profit_g)\n",
    "    pr_ge.append(pr_g)\n",
    "    sr_ge.append(sr_g)\n",
    "    ar_ge.append(ar_g)\n",
    "    ports_ge.append(port_ge)\n",
    "    final_ge.append(port_ge.iloc[-1])\n",
    "    \n",
    "bnhs_ge = (500000 * np.cumprod(1 + ge_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_ge = bnhs_ge.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_ge-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_ge))\n",
    "print('bnh ar', annual_return(bnhs_ge))\n",
    "\n",
    "print('ge profit', calc_ci(profit_ge))\n",
    "print('ge pr', calc_ci(pr_ge))\n",
    "print('ge sr', calc_ci(sr_ge))\n",
    "print('ge ar', calc_ci(ar_ge))\n",
    "\n",
    "shaded_region(ports_ge,bnhs_ge,final_ge,'ge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379e03d-1b5a-4396-916f-5b9d5c3f0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "ge_trial_runs = pd.DataFrame(ports_ge).T\n",
    "ge_trial_runs.to_csv('ge_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0941e0-155c-45ed-a3cb-a53a95ad4e26",
   "metadata": {},
   "source": [
    "## Generalization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2e090-bfc1-4ae3-ab63-91cbdcf34fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(btc_t_pre, lookback=lb, transaction =trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f43c63-9fa2-4ebc-ae0a-89f7c8e81f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1)  \n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  \n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2) \n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions] \n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  \n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"btc_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3e101-8f31-4ef5-81ec-1e5819b5abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward btc', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'btc_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995e5a9-9d5f-4ccd-912b-9c88f9cd93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_btc = []\n",
    "pr_btc = []\n",
    "sr_btc = []\n",
    "ar_btc= []\n",
    "ports_btc = []\n",
    "final_btc = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_btc = pd.DataFrame(episode_history)\n",
    "    port_btc = pr_v_btc.Portfolio\n",
    "    profit_b = profit_return(port_btc)[0]\n",
    "    pr_b = profit_return(port_btc)[1]\n",
    "    sr_b = sharpe_ratio(port_btc)\n",
    "    ar_b = annual_return(port_btc)\n",
    "    profit_btc.append(profit_b)\n",
    "    pr_btc.append(pr_b)\n",
    "    sr_btc.append(sr_b)\n",
    "    ar_btc.append(ar_b)\n",
    "    ports_btc.append(port_btc)\n",
    "    final_btc.append(port_btc.iloc[-1])\n",
    "    \n",
    "bnhs_btc = (500000 * np.cumprod(1 + btc_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_btc = bnhs_btc.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_btc-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_btc))\n",
    "print('bnh ar', annual_return(bnhs_btc))\n",
    "\n",
    "print('btc profit', calc_ci(profit_btc))\n",
    "print('btc pr', calc_ci(pr_btc))\n",
    "print('btc sr', calc_ci(sr_btc))\n",
    "print('btc ar', calc_ci(ar_btc))\n",
    "\n",
    "shaded_region(ports_btc,bnhs_btc,final_btc,'btc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf5482-9390-41c5-868e-689e8253af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "btc_trial_runs = pd.DataFrame(ports_btc).T\n",
    "btc_trial_runs.to_csv('btc_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ce9ac-2fc7-46d1-931c-fdc0014a94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_eth = []\n",
    "pr_eth = []\n",
    "sr_eth = []\n",
    "ar_eth= []\n",
    "ports_eth = []\n",
    "final_eth = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v1.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v1.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_eth = pd.DataFrame(episode_history)\n",
    "    port_eth = pr_v_eth.Portfolio\n",
    "    profit_e = profit_return(port_eth)[0]\n",
    "    pr_e = profit_return(port_eth)[1]\n",
    "    sr_e = sharpe_ratio(port_eth)\n",
    "    ar_e = annual_return(port_eth)\n",
    "    profit_eth.append(profit_e)\n",
    "    pr_eth.append(pr_e)\n",
    "    sr_eth.append(sr_e)\n",
    "    ar_eth.append(ar_e)\n",
    "    ports_eth.append(port_eth)\n",
    "    final_eth.append(port_eth.iloc[-1])\n",
    "    \n",
    "bnhs_eth = (500000 * np.cumprod(1 + eth_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_eth = bnhs_eth.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_eth-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_eth))\n",
    "print('bnh ar', annual_return(bnhs_eth))\n",
    "\n",
    "print('eth profit', calc_ci(profit_eth))\n",
    "print('eth pr', calc_ci(pr_eth))\n",
    "print('eth sr', calc_ci(sr_eth))\n",
    "print('eth ar', calc_ci(ar_eth))\n",
    "\n",
    "shaded_region(ports_eth,bnhs_eth,final_eth,'eth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cca357-6328-4124-9f79-73b7657b8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "eth_trial_runs = pd.DataFrame(ports_eth).T\n",
    "eth_trial_runs.to_csv('eth_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5076100-1efd-4ffc-9946-6274c8b41b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_sol = []\n",
    "pr_sol = []\n",
    "sr_sol = []\n",
    "ar_sol = []\n",
    "ports_sol = []\n",
    "final_sol = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v2.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v2.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_sol = pd.DataFrame(episode_history)\n",
    "    port_sol = pr_v_sol.Portfolio\n",
    "    profit_s = profit_return(port_sol)[0]\n",
    "    pr_s = profit_return(port_sol)[1]\n",
    "    sr_s = sharpe_ratio(port_sol)\n",
    "    ar_s = annual_return(port_sol)\n",
    "    profit_sol.append(profit_s)\n",
    "    pr_sol.append(pr_s)\n",
    "    sr_sol.append(sr_s)\n",
    "    ar_sol.append(ar_s)\n",
    "    ports_sol.append(port_sol)\n",
    "    final_sol.append(port_sol.iloc[-1])\n",
    "    \n",
    "bnhs_sol = (500000 * np.cumprod(1 + sol_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_sol = bnhs_sol.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_sol-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_sol))\n",
    "print('bnh ar', annual_return(bnhs_sol))\n",
    "\n",
    "print('sol profit', calc_ci(profit_sol))\n",
    "print('sol pr', calc_ci(pr_sol))\n",
    "print('sol sr', calc_ci(sr_sol))\n",
    "print('sol ar', calc_ci(ar_sol))\n",
    "\n",
    "shaded_region(ports_sol,bnhs_sol,final_sol,'sol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63503d6b-2f2c-4161-933b-eede9034ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "sol_trial_runs = pd.DataFrame(ports_sol).T\n",
    "sol_trial_runs.to_csv('sol_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d5330-3750-4c30-83d2-5f8a3d17d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_bnb = []\n",
    "pr_bnb = []\n",
    "sr_bnb = []\n",
    "ar_bnb= []\n",
    "ports_bnb = []\n",
    "final_bnb = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v3.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v3.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_bnb = pd.DataFrame(episode_history)\n",
    "    port_bnb = pr_v_bnb.Portfolio\n",
    "    profit_bn = profit_return(port_bnb)[0]\n",
    "    pr_bn = profit_return(port_bnb)[1]\n",
    "    sr_bn = sharpe_ratio(port_bnb)\n",
    "    ar_bn = annual_return(port_bnb)\n",
    "    profit_bnb.append(profit_bn)\n",
    "    pr_bnb.append(pr_bn)\n",
    "    sr_bnb.append(sr_bn)\n",
    "    ar_bnb.append(ar_bn)\n",
    "    ports_bnb.append(port_bnb)\n",
    "    final_bnb.append(port_bnb.iloc[-1])\n",
    "    \n",
    "bnhs_bnb = (500000 * np.cumprod(1 + bnb_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_bnb = bnhs_bnb.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_bnb-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_bnb))\n",
    "print('bnh ar', annual_return(bnhs_bnb))\n",
    "\n",
    "print('bnb profit', calc_ci(profit_bnb))\n",
    "print('bnb pr', calc_ci(pr_bnb))\n",
    "print('bnb sr', calc_ci(sr_bnb))\n",
    "print('bnb ar', calc_ci(ar_bnb))\n",
    "\n",
    "shaded_region(ports_bnb,bnhs_bnb,final_bnb,'bnb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a098de-7807-4950-b9b9-f9f6dfc9f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "bnb_trial_runs = pd.DataFrame(ports_bnb).T\n",
    "bnb_trial_runs.to_csv('bnb_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903d506-74a8-4259-96d7-7dca5525f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_xrp = []\n",
    "pr_xrp = []\n",
    "sr_xrp = []\n",
    "ar_xrp = []\n",
    "ports_xrp = []\n",
    "final_xrp = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v4.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v4.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_xrp = pd.DataFrame(episode_history)\n",
    "    port_xrp = pr_v_xrp.Portfolio\n",
    "    profit_x = profit_return(port_xrp)[0]\n",
    "    pr_x = profit_return(port_xrp)[1]\n",
    "    sr_x = sharpe_ratio(port_xrp)\n",
    "    ar_x = annual_return(port_xrp)\n",
    "    profit_xrp.append(profit_x)\n",
    "    pr_xrp.append(pr_x)\n",
    "    sr_xrp.append(sr_x)\n",
    "    ar_xrp.append(ar_x)\n",
    "    ports_xrp.append(port_xrp)\n",
    "    final_xrp.append(port_xrp.iloc[-1])\n",
    "    \n",
    "bnhs_xrp = (500000 * np.cumprod(1 + xrp_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_xrp = bnhs_xrp.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_xrp-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_xrp))\n",
    "print('bnh ar', annual_return(bnhs_xrp))\n",
    "\n",
    "print('xrp profit', calc_ci(profit_xrp))\n",
    "print('xrp pr', calc_ci(pr_xrp))\n",
    "print('xrp sr', calc_ci(sr_xrp))\n",
    "print('xrp ar', calc_ci(ar_xrp))\n",
    "\n",
    "shaded_region(ports_xrp, bnhs_xrp, final_xrp,'xrp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047c3b0-2ab0-462d-a130-c377f45c2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "xrp_trial_runs = pd.DataFrame(ports_xrp).T\n",
    "xrp_trial_runs.to_csv('xrp_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be86a26-7fb1-4825-bb50-41dce5ba2648",
   "metadata": {},
   "source": [
    "## Ablation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549c357-e5c4-4c15-b446-80b91bd42bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SingleStockohlcvCuilogr_no_kalman as envv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad6ad9-4d94-4425-9de0-a376274217c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 20 #window length\n",
    "vars = 5 #number of variables OHLCV\n",
    "trans = 0.001 #0.1% transaction cost\n",
    "#create all validation environments for ablation experiments\n",
    "env_v = envv2.SingleStockTraderohlcvCuilogr_no_kalman(app_v_pre, lookback=lb, transaction = trans)\n",
    "env_v2 = envv2.SingleStockTraderohlcvCuilogr_no_kalman(ge_v_pre, lookback=lb, transaction = trans)\n",
    "env_v3 = envv2.SingleStockTraderohlcvCuilogr_no_kalman(dji_v_pre, lookback=lb, transaction = trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42091def-34bc-4b0f-9ee6-149c694cafc3",
   "metadata": {},
   "source": [
    "### DJI no kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb222d66-7b94-436e-b1d0-2506b7b744d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderohlcvCuilogr_no_kalman(dji_t_pre, lookback=lb, transaction = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb9166-0ba2-431d-9d5b-6c8c4c49ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1)  \n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  \n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2) \n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions] \n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  \n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v3.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v3.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"dji_no_kf_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0c885-4e44-4875-835b-51991e80d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward dji no KF', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'dji_no_kf_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7263e-c946-4a30-a28a-92e9616fb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_dji1 = []\n",
    "pr_dji1 = []\n",
    "sr_dji1 = []\n",
    "ar_dji1 = []\n",
    "ports_dji1 = []\n",
    "final_dji1 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v3.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Get the distribution output: shape [1, action_dim, num_atoms]\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v3.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_dji1 = pd.DataFrame(episode_history)\n",
    "    port_dji1 = pr_v_dji1.Portfolio\n",
    "    profit_d1 = profit_return(port_dji1)[0]\n",
    "    pr_d1 = profit_return(port_dji1)[1]\n",
    "    sr_d1 = sharpe_ratio(port_dji1)\n",
    "    ar_d1 = annual_return(port_dji1)\n",
    "    profit_dji1.append(profit_d1)\n",
    "    pr_dji1.append(pr_d1)\n",
    "    sr_dji1.append(sr_d1)\n",
    "    ar_dji1.append(ar_d1)\n",
    "    ports_dji1.append(port_dji1)\n",
    "    final_dji1.append(port_dji1.iloc[-1])\n",
    "    \n",
    "bnhs_dji = (500000 * np.cumprod(1 + dji_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_dji = bnhs_dji.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_dji-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_dji))\n",
    "print('bnh ar', annual_return(bnhs_dji))\n",
    "\n",
    "print('dji no KF profit', calc_ci(profit_dji1))\n",
    "print('dji no KF pr', calc_ci(pr_dji1))\n",
    "print('dji no KF sr', calc_ci(sr_dji1))\n",
    "print('dji no KF ar', calc_ci(ar_dji1))\n",
    "shaded_region(ports_dji1,bnhs_dji,final_dji1,'dji no KF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eda425-9171-47d6-bd6c-9c19016adedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "dji_no_kf_trial_runs = pd.DataFrame(ports_dji1).T\n",
    "dji_no_kf_trial_runs.to_csv('dji_no_kf_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18fd25-29f9-43f3-9846-7dc1920d5cb9",
   "metadata": {},
   "source": [
    "### aaple no kf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab6f56-430f-473d-a237-32c973190b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderohlcvCuilogr_no_kalman(app_t_pre, lookback=lb, transaction = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984e656-4545-4056-97c2-402dd37f6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1)  \n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  \n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2) \n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions] \n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  \n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"app_no_kf_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb921e-7b53-4f00-a38f-787d6ba70f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward apple no KF', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'apple_no_kf_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a466b8d-1421-448b-8bcc-d5d6fbe5985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_app1 = []\n",
    "pr_app1 = []\n",
    "sr_app1 = []\n",
    "ar_app1 = []\n",
    "ports_app1 = []\n",
    "final_app1 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Get the distribution output: shape [1, action_dim, num_atoms]\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_app1 = pd.DataFrame(episode_history)\n",
    "    port_app1 = pr_v_app1.Portfolio\n",
    "    profit_a1 = profit_return(port_app1)[0]\n",
    "    pr_a1 = profit_return(port_app1)[1]\n",
    "    sr_a1 = sharpe_ratio(port_app1)\n",
    "    ar_a1 = annual_return(port_app1)\n",
    "    profit_app1.append(profit_a1)\n",
    "    pr_app1.append(pr_a1)\n",
    "    sr_app1.append(sr_a1)\n",
    "    ar_app1.append(ar_a1)\n",
    "    ports_app1.append(port_app1)\n",
    "    final_app1.append(port_app1.iloc[-1])\n",
    "    \n",
    "bnhs_app = (500000 * np.cumprod(1 + apple_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_app = bnhs_app.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_app-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_app))\n",
    "print('bnh ar', annual_return(bnhs_app))\n",
    "\n",
    "print('apple no kf profit', calc_ci(profit_app1))\n",
    "print('apple no k pr', calc_ci(pr_app1))\n",
    "print('apple no k sr', calc_ci(sr_app1))\n",
    "print('apple no k ar', calc_ci(ar_app1))\n",
    "\n",
    "shaded_region(ports_app1,bnhs_app,final_app1,'apple no k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bab34-0289-4eed-8818-4171f17642b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "app_no_kf_trial_runs = pd.DataFrame(ports_app1).T\n",
    "app_no_kf_trial_runs.to_csv('app_no_kf_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560819f-67ff-4762-bcd0-180e7d9aa6fd",
   "metadata": {},
   "source": [
    "### GE no kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f49a0e-da0b-48e8-b038-7c05d026a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderohlcvCuilogr_no_kalman(ge_t_pre, lookback=lb, transaction = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2685e42-7a7e-49a4-bf33-c4f3f5ec0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb*vars\n",
    "memory = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "n_steps = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "# C51-specific hyperparameters\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# Instantiate networks and optimizer\n",
    "policy = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = C51CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay= 1e-6)\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_end = 1\n",
    "beta_frames = max_episodes * max_steps\n",
    "\n",
    "\n",
    "replay = PER(memory, alpha=0.3)\n",
    "\n",
    "\n",
    "def projection_distribution(next_distr, rewards, dones, n_steps_arr, gamma, support, num_atoms, Vmin, Vmax):\n",
    "\n",
    "    batch_size = rewards.size(0)\n",
    "    delta_z = (Vmax - Vmin) / (num_atoms - 1)\n",
    "    multiplier = torch.pow(gamma, n_steps_arr.float()).unsqueeze(1)  \n",
    "    # Compute Tz = reward + (gamma^n)*support, zeroing out future rewards if terminal\n",
    "    Tz = rewards.unsqueeze(1) + multiplier * support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
    "    Tz = Tz.clamp(Vmin, Vmax)\n",
    "    b = (Tz - Vmin) / delta_z  # position of Tz in the support space\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, num_atoms, device=rewards.device)\n",
    "    # Distribute probability mass to nearest bins\n",
    "    for j in range(num_atoms):\n",
    "        mass = next_distr[:, j]\n",
    "        proj_dist.scatter_add_(1, l[:, j].unsqueeze(1), (mass * (u[:, j].float() - b[:, j])).unsqueeze(1))\n",
    "        proj_dist.scatter_add_(1, u[:, j].unsqueeze(1), (mass * (b[:, j] - l[:, j].float())).unsqueeze(1))\n",
    "    \n",
    "    return proj_dist\n",
    "\n",
    "start = time.time()  \n",
    "total_rewards = []\n",
    "losses = []\n",
    "valid_avg = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    multi_step_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "        #beta =1  \n",
    "        beta = min(beta_end, beta_start + (1 - beta_start) * (global_step / beta_frames))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                prob_dist = policy(state_tensor)            \n",
    "                q_values = torch.sum(prob_dist * policy.support, dim=2)  \n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "      \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "            \n",
    "        multi_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(multi_step_buffer) == n_steps:\n",
    "            G = 0\n",
    "            for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "                G += (gamma ** idx) * r\n",
    "                \n",
    "            first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "            last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "            with torch.no_grad():\n",
    "                first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                pred_prob = policy(first_state_tensor)\n",
    "                q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "                \n",
    "                last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                next_prob = target(last_next_state_tensor)\n",
    "                max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "                effective_gamma = gamma ** len(multi_step_buffer)\n",
    "                target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "                multi_step_error = abs(q_val - target_multi)\n",
    "            replay.add(first_state, first_action, G, last_next_state, last_done, len(multi_step_buffer), multi_step_error)\n",
    "            multi_step_buffer.popleft()\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "        batch = replay.sample(batch_size, beta, device=device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones, n_steps_arr, weights, indices = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next-state distributions solely from the target network\n",
    "                next_distr = target(next_states)\n",
    "                # Compute Q-values from the target network distributions\n",
    "                next_q = torch.sum(next_distr * target.support, dim=2) \n",
    "                # Select the best next action using target network Q-values\n",
    "                next_actions = torch.argmax(next_q, dim=1)\n",
    "                # Get the corresponding distribution for each transition\n",
    "                next_dist = next_distr[torch.arange(batch_size), next_actions] \n",
    "                \n",
    "                # Project the target distribution onto the fixed support\n",
    "                target_dist = projection_distribution(next_dist, rewards, dones, n_steps_arr, gamma, target.support, num_atoms, Vmin, Vmax)\n",
    "                target_dist = target_dist.clamp(min=1e-8)\n",
    "\n",
    "            \n",
    "            # Get current predicted distribution for the taken actions\n",
    "            pred_distr = policy(states)  \n",
    "            pred_distr = pred_distr.gather(1, actions.unsqueeze(1).unsqueeze(2).expand(-1, 1, num_atoms)).squeeze(1)\n",
    "            pred_distr = pred_distr.clamp(min=1e-8)\n",
    "            \n",
    "            log_pred = torch.log(pred_distr)\n",
    "            loss_per_sample = - (target_dist * log_pred).sum(dim=1)\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, loss_per_sample.detach().cpu().numpy())\n",
    "    \n",
    "    # Flush any remaining transitions in multi-step buffer\n",
    "    while multi_step_buffer:\n",
    "        n = len(multi_step_buffer)\n",
    "        G = 0\n",
    "        for idx, (_, _, r, _, _) in enumerate(multi_step_buffer):\n",
    "            G += (gamma ** idx) * r\n",
    "            \n",
    "        first_state, first_action, _, _, _ = multi_step_buffer[0]\n",
    "        last_next_state, last_done = multi_step_buffer[-1][3], multi_step_buffer[-1][4]\n",
    "        with torch.no_grad():\n",
    "            first_state_tensor = torch.tensor(first_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            pred_prob = policy(first_state_tensor)\n",
    "            q_val = torch.sum(pred_prob * policy.support, dim=2).squeeze(0)[first_action].item()\n",
    "            \n",
    "            last_next_state_tensor = torch.tensor(last_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_prob = target(last_next_state_tensor)\n",
    "            max_next_q = torch.sum(next_prob * target.support, dim=2).max(1)[0].item()\n",
    "            effective_gamma = gamma ** len(multi_step_buffer)\n",
    "            target_multi = G + effective_gamma * (1 - int(last_done)) * max_next_q\n",
    "            multi_step_error = abs(q_val - target_multi)\n",
    "    \n",
    "        replay.add(first_state, first_action, G, last_next_state, last_done, n, multi_step_error)\n",
    "        multi_step_buffer.popleft()\n",
    "        \n",
    "   \n",
    "    if global_step % 2000 == 0:\n",
    "        target.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    \n",
    "   \n",
    "    val_state = env_v2.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        val_state_tensor = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            val_prob = policy(val_state_tensor)\n",
    "            val_q = torch.sum(val_prob * policy.support, dim=2)\n",
    "            val_action = torch.argmax(val_q, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v2.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "        \n",
    "    s = time.time()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, Training Reward: {ep_reward:.2f}, ' +\n",
    "              f'Avg Training Reward: {np.mean(total_rewards):.2f}')\n",
    "        print(f' Validation Reward: {val_ep_reward:.2f}, ' +\n",
    "              f'Avg Validation Reward: {np.mean(valid_avg):.2f}')\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # every 25 episodes, save policy weights\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"ge_no_kf_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    \n",
    "end = time.time()        \n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977ca4b-b08e-4390-bff4-8797f6545cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward ge no KF', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'ge_no_kf_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee2152-849e-40a2-b978-ce1bee62ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_ge1 = []\n",
    "pr_ge1 = []\n",
    "sr_ge1 = []\n",
    "ar_ge1 = []\n",
    "ports_ge1 = []\n",
    "final_ge1 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v2.reset()[0]\n",
    "    state = norm(state)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "    \n",
    "    # Single episode run using C51\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Get the distribution output: shape [1, action_dim, num_atoms]\n",
    "        prob_dist = policy(state_tensor)\n",
    "        # Compute expected Q-values: sum(prob * support) over atoms\n",
    "        q_values = torch.sum(prob_dist * policy.support, dim=2)\n",
    "        # Choose the action with the highest expected Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _, info = env_v2.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_ge1 = pd.DataFrame(episode_history)\n",
    "    port_ge1 = pr_v_ge1.Portfolio\n",
    "    profit_g1 = profit_return(port_ge1)[0]\n",
    "    pr_g1 = profit_return(port_ge1)[1]\n",
    "    sr_g1 = sharpe_ratio(port_ge1)\n",
    "    ar_g1 = annual_return(port_ge1)\n",
    "    profit_ge1.append(profit_g1)\n",
    "    pr_ge1.append(pr_g1)\n",
    "    sr_ge1.append(sr_g1)\n",
    "    ar_ge1.append(ar_g1)\n",
    "    ports_ge1.append(port_ge1)\n",
    "    final_ge1.append(port_ge1.iloc[-1])\n",
    "    \n",
    "bnhs_ge = (500000 * np.cumprod(1 + ge_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_ge = bnhs_ge.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_ge-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_ge))\n",
    "print('bnh ar', annual_return(bnhs_ge))\n",
    "\n",
    "print('ge no kf profit', calc_ci(profit_ge1))\n",
    "print('ge no kf pr', calc_ci(pr_ge1))\n",
    "print('ge no kf sr', calc_ci(sr_ge1))\n",
    "print('ge no kf ar', calc_ci(ar_ge1))\n",
    "\n",
    "shaded_region(ports_ge1,bnhs_ge, final_ge1,'ge no kf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d54c7-9362-4d27-b743-6d5b5749122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "ge_no_kf_trial_runs = pd.DataFrame(ports_ge1).T\n",
    "ge_no_kf_trial_runs.to_csv('ge_no_kf_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc22b0-e93d-458d-a4dc-21277541df8a",
   "metadata": {},
   "source": [
    "## Ablation 2 No MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256971e1-2f4b-4cf6-aa2c-a8daf6ea5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN_CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lb: int,\n",
    "        num_vars: int,\n",
    "        hidden: int = 64,\n",
    "        hidden2: int = 32,\n",
    "        dropout_p: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lb = lb\n",
    "        self.num_vars = num_vars\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "\n",
    "        # 1) CNN encoder\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(num_vars, hidden, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(hidden, hidden2, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "        # 2) MLP head with dropout\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc1 = nn.Linear(hidden2, hidden2)\n",
    "        self.fc2 = nn.Linear(hidden2, action_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor):\n",
    "        B = s.size(0)       \n",
    "\n",
    "        # reshape market -> (B, num_vars, lb)\n",
    "        mkt = s.view(B, self.lb, self.num_vars).permute(0, 2, 1)\n",
    "\n",
    "        # 1) CNN encoder\n",
    "        x = self.conv_net(mkt)  \n",
    "\n",
    "        # 2) MLP head\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c36216-075a-4e2c-bce4-96fd40f6c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SingleStockKFohlcvCuilogr as envv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac1162-09a4-496a-9893-a69949b9b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 20 #window length\n",
    "vars = 5 #number of variables OHLCV\n",
    "trans = 0.001 #0.1% transaction cost\n",
    "env_v = envv2.SingleStockTraderKFohlcvCuilogr(app_v_pre, lookback=lb, transaction = trans)\n",
    "env_v2 = envv2.SingleStockTraderKFohlcvCuilogr(ge_v_pre, lookback=lb, transaction = trans)\n",
    "env_v3 = envv2.SingleStockTraderKFohlcvCuilogr(dji_v_pre, lookback=lb, transaction = trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3c185-30a4-4c66-b035-01a6ad591326",
   "metadata": {},
   "source": [
    "### Apple no MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437869ee-52d7-4fc3-813b-4d0924c4bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(app_t_pre, lookback=lb, transaction =trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f0a98-bcb0-4763-840a-452bf9089582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb * vars\n",
    "buffer_capacity = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "target_update_every = 2000  # steps\n",
    "\n",
    "#Simple Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store as numpy for memory efficiency\n",
    "        self.buffer.append((\n",
    "            np.array(state, dtype=np.float32),\n",
    "            int(action),\n",
    "            float(reward),\n",
    "            np.array(next_state, dtype=np.float32),\n",
    "            bool(done)\n",
    "        ))\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.stack(states), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(np.stack(next_states), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "policy = DQN_CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = DQN_CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay=1e-6)\n",
    "\n",
    "replay = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "total_rewards = []\n",
    "valid_avg = []\n",
    "losses = [] \n",
    "global_step = 0\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_vals = policy(s)  \n",
    "                action = torch.argmax(q_vals, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "\n",
    "        replay.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # Sample and learn\n",
    "        batch = replay.sample(batch_size, device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "            # Current Q(s,a)\n",
    "            q_pred = policy(states).gather(1, actions.unsqueeze(1)).squeeze(1)  \n",
    "\n",
    "            # DDQN target:\n",
    "            with torch.no_grad():\n",
    "                next_q_policy = policy(next_states)                 \n",
    "                next_actions = torch.argmax(next_q_policy, dim=1)   \n",
    "                next_q_target = target(next_states)                 \n",
    "                next_q = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)  \n",
    "                q_target = rewards + gamma * (1.0 - dones) * next_q\n",
    "\n",
    "            \n",
    "            loss = F.mse_loss(q_pred, q_target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Periodic hard target update by steps\n",
    "        if global_step % target_update_every == 0:\n",
    "            target.load_state_dict(policy.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # epsilon decay per episode\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "    #validation\n",
    "    val_state = env_v.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_vals = policy(s)\n",
    "            val_action = torch.argmax(q_vals, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "    s = time.time()\n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, '\n",
    "              f'Train Reward: {ep_reward:.2f}, Avg Train: {np.mean(total_rewards):.2f}, '\n",
    "              f'Val: {val_ep_reward:.2f}, Avg Val: {np.mean(valid_avg):.2f}')\n",
    "\n",
    "    # Save every 25 episodes\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"app_ddqn_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    if (ep + 1) % 25 == 0:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "    ax[0].plot(total_rewards)\n",
    "    ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "    ax[0].set_title('training reward')\n",
    "    ax[1].plot(valid_avg)\n",
    "    ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "    ax[1].set_title('validation reward')\n",
    "    plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2d520-99e7-4b05-be7b-a2e877d1d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward apple no MPC', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'apple_no_mpc_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d7f23-1f04-44f0-94e4-fdd4c723e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_app2 = []\n",
    "pr_app2 = []\n",
    "sr_app2 = []\n",
    "ar_app2 = []\n",
    "ports_app2 = []\n",
    "final_app2 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v.reset()[0]         \n",
    "    state = norm(state)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_values = policy(state_tensor)              \n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, info = env_v.step(action)\n",
    "        next_state = norm(next_state)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "    pr_v_app2 = pd.DataFrame(episode_history)\n",
    "    port_app2 = pr_v_app2.Portfolio\n",
    "    profit_a2 = profit_return(port_app2)[0]\n",
    "    pr_a2 = profit_return(port_app2)[1]\n",
    "    sr_a2 = sharpe_ratio(port_app2)\n",
    "    ar_a2 = annual_return(port_app2)\n",
    "    profit_app2.append(profit_a2)\n",
    "    pr_app2.append(pr_a2)\n",
    "    sr_app2.append(sr_a2)\n",
    "    ar_app2.append(ar_a2)\n",
    "    ports_app2.append(port_app2)\n",
    "    final_app2.append(port_app2.iloc[-1])\n",
    "    \n",
    "bnhs_app = (500000 * np.cumprod(1 + apple_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_app = bnhs_app.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_app-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_app))\n",
    "print('bnh ar', annual_return(bnhs_app))\n",
    "\n",
    "print('apple no MPC profit', calc_ci(profit_app2))\n",
    "print('apple no MPC pr', calc_ci(pr_app2))\n",
    "print('apple no MPC sr', calc_ci(sr_app2))\n",
    "print('apple no MPC ar', calc_ci(ar_app2))\n",
    "\n",
    "shaded_region(ports_app2,bnhs_app,final_app2,'apple no MPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e64be2-1cd5-4d27-a8ca-2ac64eb55fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "app_no_mpc_trial_runs = pd.DataFrame(ports_app2).T\n",
    "app_no_mpc_trial_runs.to_csv('app_no_mpc_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119e07f-c74f-4f4e-b1f3-53d63e4c5118",
   "metadata": {},
   "source": [
    "### GE no MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b93782-2cbb-44cf-b52f-a5befb2f2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(ge_t_pre, lookback=lb, transaction =trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44bfcf-fb6b-4933-92af-a1d90d3cf42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb * vars\n",
    "buffer_capacity = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "target_update_every = 2000  # steps\n",
    "\n",
    "#Simple Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store as numpy for memory efficiency\n",
    "        self.buffer.append((\n",
    "            np.array(state, dtype=np.float32),\n",
    "            int(action),\n",
    "            float(reward),\n",
    "            np.array(next_state, dtype=np.float32),\n",
    "            bool(done)\n",
    "        ))\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.stack(states), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(np.stack(next_states), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "policy = DQN_CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = DQN_CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay=1e-6)\n",
    "\n",
    "replay = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "total_rewards = []\n",
    "valid_avg = []\n",
    "losses = [] \n",
    "global_step = 0\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_vals = policy(s)  \n",
    "                action = torch.argmax(q_vals, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "\n",
    "        replay.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # Sample and learn\n",
    "        batch = replay.sample(batch_size, device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "            # Current Q(s,a)\n",
    "            q_pred = policy(states).gather(1, actions.unsqueeze(1)).squeeze(1)  \n",
    "\n",
    "            # DDQN target:\n",
    "            with torch.no_grad():\n",
    "                next_q_policy = policy(next_states)                 \n",
    "                next_actions = torch.argmax(next_q_policy, dim=1)   \n",
    "                next_q_target = target(next_states)                 \n",
    "                next_q = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)  \n",
    "                q_target = rewards + gamma * (1.0 - dones) * next_q\n",
    "\n",
    "            \n",
    "            loss = F.mse_loss(q_pred, q_target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Periodic hard target update by steps\n",
    "        if global_step % target_update_every == 0:\n",
    "            target.load_state_dict(policy.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # epsilon decay per episode\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "    #validation\n",
    "    val_state = env_v2.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_vals = policy(s)\n",
    "            val_action = torch.argmax(q_vals, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v2.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "    s = time.time()\n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, '\n",
    "              f'Train Reward: {ep_reward:.2f}, Avg Train: {np.mean(total_rewards):.2f}, '\n",
    "              f'Val: {val_ep_reward:.2f}, Avg Val: {np.mean(valid_avg):.2f}')\n",
    "\n",
    "    # Save every 25 episodes\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"ge_ddqn_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950c8ce-d930-4944-ae63-0b0c85046dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward ge no MPC', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'ge_no_mpc_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae6a3b-3617-4fc1-908e-f31ab33161ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_ge2 = []\n",
    "pr_ge2 = []\n",
    "sr_ge2 = []\n",
    "ar_ge2 = []\n",
    "ports_ge2 = []\n",
    "final_ge2 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v2.reset()[0]         \n",
    "    state = norm(state)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_values = policy(state_tensor)              \n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, info = env_v2.step(action)\n",
    "        next_state = norm(next_state)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "    \n",
    "  \n",
    "    pr_v_ge2 = pd.DataFrame(episode_history)\n",
    "    port_ge2 = pr_v_ge2.Portfolio\n",
    "    profit_g2 = profit_return(port_ge2)[0]\n",
    "    pr_g2 = profit_return(port_ge2)[1]\n",
    "    sr_g2 = sharpe_ratio(port_ge2)\n",
    "    ar_g2 = annual_return(port_ge2)\n",
    "    profit_ge2.append(profit_g2)\n",
    "    pr_ge2.append(pr_g2)\n",
    "    sr_ge2.append(sr_g2)\n",
    "    ar_ge2.append(ar_g2)\n",
    "    ports_ge2.append(port_ge2)\n",
    "    final_ge2.append(port_ge2.iloc[-1])\n",
    "    \n",
    "bnhs_ge = (500000 * np.cumprod(1 + ge_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_ge = bnhs_ge.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_ge-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_ge))\n",
    "print('bnh ar', annual_return(bnhs_ge))\n",
    "\n",
    "print('ge no MPC profit', calc_ci(profit_ge2))\n",
    "print('ge no MPC pr', calc_ci(pr_ge2))\n",
    "print('ge no MPC sr', calc_ci(sr_ge2))\n",
    "print('ge no MPC ar', calc_ci(ar_ge2))\n",
    "\n",
    "shaded_region(ports_ge2,bnhs_ge, final_ge2,'ge no MPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00812f92-d465-4215-a1fb-fc526bc2222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "ge_no_mpc_trial_runs = pd.DataFrame(ports_ge2).T\n",
    "ge_no_mpc_trial_runs.to_csv('ge_no_mpc_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb01be-88a0-4933-8f35-1612c5156d7c",
   "metadata": {},
   "source": [
    "### DJI no MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e678c51-e67d-4b51-ae62-ca8fbe890468",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envv2.SingleStockTraderKFohlcvCuilogr(dji_t_pre, lookback=lb, transaction =trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e70ea-8036-4693-a45c-90d7e8802f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "gamma = 0.95\n",
    "action_dim = env.action_space.n\n",
    "state_dim = lb * vars\n",
    "buffer_capacity = 25000\n",
    "batch_size = 8\n",
    "max_episodes = 200\n",
    "max_steps = 315\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_lr = 1e-3\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.98\n",
    "\n",
    "target_update_every = 2000  # steps\n",
    "\n",
    "#Simple Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store as numpy for memory efficiency\n",
    "        self.buffer.append((\n",
    "            np.array(state, dtype=np.float32),\n",
    "            int(action),\n",
    "            float(reward),\n",
    "            np.array(next_state, dtype=np.float32),\n",
    "            bool(done)\n",
    "        ))\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.stack(states), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(np.stack(next_states), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "policy = DQN_CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target = DQN_CNN(state_dim, action_dim, lb, vars).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(), lr=init_lr, weight_decay=1e-6)\n",
    "\n",
    "replay = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "total_rewards = []\n",
    "valid_avg = []\n",
    "losses = [] \n",
    "global_step = 0\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    e = time.time()\n",
    "    state = env.reset(random_start=True, steps=max_steps)[0]\n",
    "    state = norm(state)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_vals = policy(s)  \n",
    "                action = torch.argmax(q_vals, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = norm(next_state)\n",
    "        ep_reward += reward\n",
    "\n",
    "        replay.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # Sample and learn\n",
    "        batch = replay.sample(batch_size, device)\n",
    "        if batch is not None:\n",
    "            states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "            # Current Q(s,a)\n",
    "            q_pred = policy(states).gather(1, actions.unsqueeze(1)).squeeze(1)  \n",
    "\n",
    "            # DDQN target:\n",
    "            with torch.no_grad():\n",
    "                next_q_policy = policy(next_states)                 \n",
    "                next_actions = torch.argmax(next_q_policy, dim=1)   \n",
    "                next_q_target = target(next_states)                 \n",
    "                next_q = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)  \n",
    "                q_target = rewards + gamma * (1.0 - dones) * next_q\n",
    "\n",
    "            \n",
    "            loss = F.mse_loss(q_pred, q_target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Periodic hard target update by steps\n",
    "        if global_step % target_update_every == 0:\n",
    "            target.load_state_dict(policy.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # epsilon decay per episode\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "    #validation\n",
    "    val_state = env_v3.reset(random_start=False)[0]\n",
    "    val_state = norm(val_state)\n",
    "    val_done = False\n",
    "    val_ep_reward = 0\n",
    "    while not val_done:\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(val_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_vals = policy(s)\n",
    "            val_action = torch.argmax(q_vals, dim=1).item()\n",
    "        val_next_state, val_reward, val_done, _, _ = env_v3.step(val_action)\n",
    "        val_next_state = norm(val_next_state)\n",
    "        val_ep_reward += val_reward\n",
    "        val_state = val_next_state\n",
    "    valid_avg.append(val_ep_reward)\n",
    "\n",
    "    s = time.time()\n",
    "    if ep % 10 == 0:\n",
    "        print(f'Episode {ep}, Time {(s - e):.2f}s, '\n",
    "              f'Train Reward: {ep_reward:.2f}, Avg Train: {np.mean(total_rewards):.2f}, '\n",
    "              f'Val: {val_ep_reward:.2f}, Avg Val: {np.mean(valid_avg):.2f}')\n",
    "\n",
    "    # Save every 25 episodes\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        ckpt_path = f\"dji_ddqn_policy_ep{ep+1:03d}.pth\"\n",
    "        torch.save(policy.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "        ax[0].plot(total_rewards)\n",
    "        ax[0].plot(pd.Series(total_rewards).rolling(30, min_periods=1).mean())\n",
    "        ax[0].set_title('training reward')\n",
    "        ax[1].plot(valid_avg)\n",
    "        ax[1].plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean())\n",
    "        ax[1].set_title('validation reward')\n",
    "        plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print('completed in', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d5ccc-cfff-4b55-a592-28a3fa5689d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(valid_avg, label='valid reward dji no mpc', alpha=0.5)\n",
    "plt.plot(pd.Series(valid_avg).rolling(30, min_periods=1).mean(), label = 'valid smoothed reward', c='black', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode Number', fontsize = 16)\n",
    "plt.ylabel('Episode Reward', fontsize = 16)\n",
    "plt.savefig(f'dji_no_mpc_valid_graph.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b003ff-9dae-4168-9fdb-a9d58c920344",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_dji2 = []\n",
    "pr_dji2 = []\n",
    "sr_dji2 = []\n",
    "ar_dji2 = []\n",
    "ports_dji2 = []\n",
    "final_dji2 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Reset environment\n",
    "    state = env_v3.reset()[0]         \n",
    "    state = norm(state)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_history = []\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_values = policy(state_tensor)              \n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, info = env_v3.step(action)\n",
    "        next_state = norm(next_state)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_history.append(info)\n",
    "        state = next_state\n",
    "  \n",
    "    pr_v_dji2 = pd.DataFrame(episode_history)\n",
    "    port_dji2 = pr_v_dji2.Portfolio\n",
    "    profit_d2 = profit_return(port_dji2)[0]\n",
    "    pr_d2 = profit_return(port_dji2)[1]\n",
    "    sr_d2 = sharpe_ratio(port_dji2)\n",
    "    ar_d2 = annual_return(port_dji2)\n",
    "    profit_dji2.append(profit_d2)\n",
    "    pr_dji2.append(pr_d2)\n",
    "    sr_dji2.append(sr_d2)\n",
    "    ar_dji2.append(ar_d2)\n",
    "    ports_dji2.append(port_dji2)\n",
    "    final_dji2.append(port_dji2.iloc[-1])\n",
    "    \n",
    "bnhs_dji = (500000 * np.cumprod(1 + dji_valid[lb-1:].Close.pct_change().dropna()))\n",
    "bnh_dji = bnhs_dji.iloc[-1]\n",
    "\n",
    "#calculate the CI for all evulation metrics \n",
    "print('bnh profit', bnh_dji-500000)\n",
    "print('bnh sr',sharpe_ratio(bnhs_dji))\n",
    "print('bnh ar', annual_return(bnhs_dji))\n",
    "\n",
    "print('dji no MPC profit', calc_ci(profit_dji2))\n",
    "print('dji no MPC pr', calc_ci(pr_dji2))\n",
    "print('dji no MPC sr', calc_ci(sr_dji2))\n",
    "print('dji no MPC ar', calc_ci(ar_dji2))\n",
    "shaded_region(ports_dji2,bnhs_dji,final_dji2,'dji no MPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430aa37-e5fd-49f8-a197-55060e1f2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ind runs if needed for future work or graph tweaking\n",
    "dji_no_mpc_trial_runs = pd.DataFrame(ports_dji2).T\n",
    "dji_no_mpc_trial_runs.to_csv('dji_no_mpc_trial_runs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2d53a-e9d4-4778-8bca-6f550771d1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
